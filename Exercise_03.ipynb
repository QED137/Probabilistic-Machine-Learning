{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "#### Machine Learning in Science, University of TÃ¼bingen, Summer Semester 2024\n",
    "## Exercise 03\n",
    "\n",
    "**hand in before 9.05.2022, 8:00 a.m.**\n",
    "\n",
    "---\n",
    "\n",
    "In the lecture you learned about Gaussian linear regression and how it is used to infer a functional relationship underlying the observed data. Gaussian linear regression takes a probabilistic view; it defines a prior over the weights of a candicate function, which implictly defines a prior over all candicate functions. By performing Bayesian inference we can then obtain the posterior over the weights and the corresponding functions. When everything is set up to be Gaussian, the posterior is available in analytical form given by a series of linear algebra operations. \n",
    "\n",
    "In the exercise sheet you explored the differences between the maximum likelihood estimate (MLE) of linear regression, the probabilistic view of Gaussian linear regression, and the connection of regularized linear regression and the choice of prior. Here in the programming exercise, you will implement and compare these different approaches in practice. \n",
    "\n",
    "Your overall goal is to infer the functional relationship underlying some observed data. \n",
    "At first, you will do so by implementing the non-probabilistic formulation of linear regression, i.e. the MLE of Gaussian linear regression, also known as the \"normal equation\" or \"[ordinary least squares  (Wikipedia)](https://en.wikipedia.org/wiki/Ordinary_least_squares)\". Second, you will implement Gaussian linear regression in the the fully probabilistic view. \n",
    "Finally, you will compare the different approaches that you derived in the exercise sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports are all you need to complete the exercise.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from numpy.random import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load and plot the observed data\n",
    "\n",
    "Load the data saved in the file `observed_data.p` (e.g., using `pickle`). It contains your observation in the form of `X` values and `y` values as type `np.array`, as well as the observation noise `sigma` as type `float`. Your first task is to visualize the data in a 2D plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here...\n",
    "\n",
    "# X, y, sigma = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Linear regression with the normal equation\n",
    "\n",
    "#### Choosing a feature function\n",
    "You first need to define a feature function $\\phi: X \\to \\mathbb{R}$ that maps the observed data to the feature space. \n",
    "Note that this is an initial design choice your have to make. You need to decide on a set of features, and how many of them you use.\n",
    "\n",
    "For example, you could decide to use polynomial features up to degree K, i.e., you use [$x, x^2, \\ldots, x^K$], for some $K \\in \\mathbb{N}$. \n",
    "But you could use any other set of features, e.g., Fourier features, step functions, Gaussians... \n",
    "\n",
    "Once you have settled on a feature function, you can apply it to your observed data. Your feature function together with the weight applied to every feature define your model of the observed target variables:\n",
    "\n",
    "$$\n",
    "f(x) = \\phi_X{\\top}w\n",
    "$$\n",
    "\n",
    "Following the normal equation you derived in exercise 2(a), complete the following tasks: \n",
    "\n",
    "##### Tasks\n",
    "- Calculate the MLE of the weights.\n",
    "- Obtain the MLE prediction of the underlying function by applying calculated weights to the features.\n",
    "- Plot the resulting function on top the observed data.\n",
    "- Does the function fit the data well? Play around with the type and the number of features to fit the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Gaussian linear regression\n",
    "\n",
    "Next, we take the probabilistic view perform inference over the weights of our candidate function (instead of making a maximum likelihood point estimate). We use the same prior setting as in the exercise sheet: zero mean Gaussian prior with unit covariance matrix. \n",
    "\n",
    "Using the equations presented in the lecture, and derived by you in the exercise sheet (exercise 1), perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Visualize the prior over functions\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- Define the prior mean and covariance in `numpy`.\n",
    "- Define the implied prior over your candidate functions as a `numpy` Gaussian distribution (imported above). As data you could use your observed data, but for better visualization we recommend using just a uniformly spaced 1D grid of `x`, e.g., `x = np.linspace(-1, 1, 1000)`.\n",
    "- Draw samples from the prior over function and visualize them in `(x, f(x))` space. \n",
    "- Plot the marginal *standard deviation* of the prior over functions on top (hint: the marginal variance is given by the diagonal elements of the covariance matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) Obtain the posterior over $w$ and over functions $f$\n",
    "\n",
    "The posterior over $w$ and the implied posterior over $f$ are both given in closed form and are Gaussian. Use the formulas from the lecture and/or your exercises to calculate and to visualize them, by completing the following tasks. Note that this posterior should be with respect to your observed data `X`. \n",
    "\n",
    "Tasks:\n",
    "- Construct the posterior over $w$ using equations from lecture, e.g., obtain the mean $\\mu_w$ and covariance $\\Sigma_w$.\n",
    "- Construct the posterior on f by obtaining the mean $\\mu_f$ and the covariance $\\Sigma_f$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3) Generate and plot posterior samples for $w$ and $f$\n",
    "- Sample functions from the posterior over $f$, and visualize them together with your observed data `(X, y)`. \n",
    "- Add the posterior mean $\\mu_f$ and the marginal standard deviation to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the marginal standard deviation of the posterior to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4) How do the inferred functions look like outside the data region? \n",
    "\n",
    "Functions sampled from the posterior should fit the data well, but how do they look like outside the ranges of the observed data?\n",
    "\n",
    "To visualize this you need to evaluate the functions sampled from the posterior w.r.t to `X` over a new range of test values, e.g., from a 1D grid `x = np.linspace(-2, 3, 1000)` (hint: you need to sample the posterior over $w$). \n",
    "\n",
    "Tasks:\n",
    "- generate a 1D grid of test observations for visualization\n",
    "- plot the functions corresponding the samples from the posterior w.r.t to the observed data `X` (see hint above)\n",
    "- How do you interpret the result? Is the behaviour of the inferred functions outside the data ranges reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Comparison\n",
    "\n",
    "In exercise `(2)` of the exercise sheet you studied the correspondence between the choice of prior and the kind of regularization in linear regression. Let's check this in practice: How do the MLE and probabilistic solution compare to each other? What are the conceptual differences? Which fit do you think is better, and why? \n",
    "\n",
    "- Show visually that the posterior mean coincides with the l2-regularized least-squares estimator (ridge regression point estimate).\n",
    "- How would you have to change the prior in order to implicitly regularize the weights with a LASSO regularization? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ef9b53a5ce850816b9705a866e49207a37a04a71269aa157d9f9ab944ea42bf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
