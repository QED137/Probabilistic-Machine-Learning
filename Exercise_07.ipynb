{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6000b5c8-9f23-478e-be3b-a950c7698d17",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "#### Machine Learning in Science, University of Tübingen, Summer Semester 2024\n",
    "## Exercise 07\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In the lecture you learned about different methods that allow sampling from complex distributions. In this programming exercise, you will explore importance sampling in order to refine laplace approximations and implement a Gibbs-sampler for an Ising model.\n",
    "\n",
    "\n",
    "## Outline\n",
    "1) Refining Laplace approximations with importance sampling.\n",
    "\n",
    "2) Gibbs-sampling for Ising model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2ad35",
   "metadata": {},
   "source": [
    "# Refining Laplace approximations with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5887ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.distributions import Normal, Uniform, Bernoulli\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc043a-2334-4769-9b38-c33b67ae6ba0",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we will refine a laplace approximation with importance sampling. Our goal will be to estimate the posterior mean $\\mathbb{E}[p(y|x)]$ with Monte-Carlo sampling. We will show that the laplace approximation can fail to estimate this mean accurately, and that importance sampling can improve the estimate. First, we define a simple model $p(y|x)$ and a prior $p(x)$. The likelihood is $p(y | x) = \\mathcal{N}(\\mu=30 x + \\frac{1}{x}, \\sigma=0.5)$. The prior is Uniform $p(x) = U(0.01, 1.0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d374b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dist = Normal(0, 0.5)\n",
    "\n",
    "def log_likelihood(x):\n",
    "    simulation = 30*x + 1/(x)\n",
    "    difference = simulation - y_o\n",
    "    return noise_dist.log_prob(difference)\n",
    "\n",
    "prior = Uniform(0.01, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6bda1-fbdc-465d-85d5-b1a9c9d1249b",
   "metadata": {},
   "source": [
    "Let's visualize this model by evaluating the probabilities on a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ad3ca-270a-4f5d-81ac-50e21d2fcf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = torch.linspace(0.01, 1, 1000)\n",
    "y_grid = torch.linspace(0, 50, 100)\n",
    "\n",
    "all_likelihoods = []\n",
    "for y_ in y_grid:\n",
    "    y_o = y_\n",
    "    likelihoods = torch.exp(log_likelihood(x_grid))\n",
    "    all_likelihoods.append(likelihoods)\n",
    "all_likelihoods = torch.stack(all_likelihoods)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "plt.imshow(all_likelihoods.numpy(), origin=\"lower\", extent=[0.01, 1, 0, 50], aspect=\"auto\")\n",
    "_ = ax.set_xlabel(\"x\")\n",
    "_ = ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87de74-bed1-4fa1-ac56-b0075a757ca0",
   "metadata": {},
   "source": [
    "Assume the observed data is $y_o = 11.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d19cd0-e2e4-4d4c-ab5d-bcd31f148aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_o = 11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41257036-8df0-4dcb-a4f1-ffe48124f7ac",
   "metadata": {},
   "source": [
    "Then, given likelihood and prior, we can compute the unnormalized posterior $p(x | y_o) \\propto p(y_o | x)p(x)$. Implement such a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38d87e-bb4d-4f75-b82d-50e216c1e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalized_log_posterior(x):\n",
    "    return ...\n",
    "# If implemented correctly, this should return:\n",
    "# unnormalized_log_posterior(torch.tensor([0.5])) -> tensor([-67.4958])\n",
    "# unnormalized_log_posterior(torch.tensor([0.6, 0.8])) -> tensor([-143.5846, -395.0208])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0d3c3-78a8-4042-9592-f5e4ab18d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalized_posterior(x):\n",
    "    return torch.exp(unnormalized_log_posterior(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5539d2-7432-4f69-8fe3-03a656ffaa1b",
   "metadata": {},
   "source": [
    "### Establishing a ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e2b45-b67b-46a0-923e-b61df4a468a4",
   "metadata": {},
   "source": [
    "In this simple model, we can even compute the normalization constant by numerical integration (on a grid). We will use this to obtain a ground truth for the posterior later on. Note that, in more challenging real-world scenarios, this will not usually be possible. There are no tasks for you to do in this section, but please make sure that you understand the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 0.99 / 999\n",
    "l = unnormalized_posterior(x_grid)\n",
    "Z_true = torch.sum(l) * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b70e15-78ba-473a-bdd6-eca16810bb18",
   "metadata": {},
   "source": [
    "After having estimated the normalization constant, we can write a function that allows us to evaluate the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ecd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(x):\n",
    "    return unnormalized_posterior(x) / Z_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b351be4-098d-4f4e-a112-753364f55622",
   "metadata": {},
   "source": [
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f55148",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = torch.linspace(0.01, 1, 1000)\n",
    "posterior_values_on_grid = posterior(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,3))\n",
    "_ = ax.plot(x, posterior_values_on_grid.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1938f8-6383-4f02-816a-6b56984fad46",
   "metadata": {},
   "source": [
    "In order to get a ground-truth value for the posterior mean, we compute the expected value of this posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0109c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_val = x_grid * posterior(x_grid)\n",
    "mean_true = torch.sum(func_val) * dx\n",
    "print(\"Mean when integrating on grid: \", mean_true.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f0d2b-b138-4de7-ba11-7f7eae0a214a",
   "metadata": {},
   "source": [
    "### Laplace approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171afef4-5f9c-49d9-9b28-5afd63435d50",
   "metadata": {},
   "source": [
    "In almost any real-world scenario, we can not compute the normalization constant of the posterior and we, therefore, can not evaluate the posterior mean as done above. Instead, we have to rely on other methods to approximate the posterior or generate (approximate) samples from it.\n",
    "\n",
    "We will now explore a laplace-approximation to the posterior. Write a function to obtain the Maximum-likelihood-estimate $x_{\\text{MLE}} = \\min_{x} -\\log p(x | y)$ with gradient descent. As starting point, use $x = 0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mle = ...\n",
    "optim = ...\n",
    "\n",
    "for _ in range(500):\n",
    "        optim.zero_grad()\n",
    "        loss = ...\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "# If implemented correctly, the x_mle should be\n",
    "# Parameter containing:\n",
    "# tensor([0.2270], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83fd238-6ca6-4452-b5a6-15cb98f73262",
   "metadata": {},
   "source": [
    "Compute the hessian of the unnormalized log-posterior around the maximum likelihood estimate. Use the pytorch method `hessian`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = ...\n",
    "# If implemented correctly, the hessian should be:\n",
    "# tensor([[-458.7795]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12866c-146f-4231-8593-649fe845c979",
   "metadata": {},
   "source": [
    "...and construct the laplace approximation $\\mathcal{N}(\\mu=x_{\\text{MLE}}, \\sigma=\\sqrt{-H^{-1}})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_approximation = torch.distributions.Normal(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2835d09-b8b2-41e9-ac6a-a92f3c735ec5",
   "metadata": {},
   "source": [
    "We can again visualize this distribution and compare it to the (in this simple case available) ground truth posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ace57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_approximation_values_on_grid = torch.exp(laplace_approximation.log_prob(x_grid)).squeeze()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,3))\n",
    "_ = ax.plot(x_grid, posterior_values_on_grid.detach().numpy())\n",
    "_ = ax.plot(x_grid, laplace_approximation_values_on_grid.detach().numpy())\n",
    "_ = ax.legend([\"True posterior\", \"Laplace approximation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b303f-8f1a-4779-bdc7-1ff17ca660ba",
   "metadata": {},
   "source": [
    "From the above plot, we can already guess that the mean of the laplace approximation will be higher than the true posterior mean. Since the Laplace approximation is a gaussian, it's mean is the maximum-likelihood estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf77e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_mean = laplace_approximation.mean\n",
    "print(\"Mean as estimated by laplace: \", laplace_mean.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf83b4",
   "metadata": {},
   "source": [
    "### Refine with importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a512eef-5da3-49b1-9981-a0771b0381cb",
   "metadata": {},
   "source": [
    "The laplace approximation overestimated the posterior mean. Let's see if we can fix this with importance sampling. We will use the Laplace approximation as proposal distribution $q(x)$. We then compute:\n",
    "\n",
    "$\\mathbb{E}[p(x | y)] = \\int p(x | y) \\; x \\; dx = \\int \\frac{p(x | y)}{q(x)} \\; x \\; q(x) \\; dx \\approx \\frac{1}{N} \\sum_i \\frac{p(x_i | y)}{q(x_i)} x_i$.\n",
    "\n",
    "However, in the current scenario, the we can only estimate the posterior distribution **up to proportionality**. Therefore, we first have to estimate the normalization constant $Z$ of the unnormalized posterior $\\tilde{p}(x | y)$. We again do so with importance sampling:\n",
    "\n",
    "$Z = \\int \\tilde{p}(x | y) dx = \\int \\frac{\\tilde{p}(x | y)}{q(x)} q(x) dx \\approx \\frac{1}{N} \\sum_i \\frac{\\tilde{p}(x_i | y)}{q(x_i)}$. Let's first estimate $Z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d40414",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_samples = ...\n",
    "laplace_log_probs = ...\n",
    "laplace_probs = torch.exp(laplace_log_probs)\n",
    "\n",
    "unnormalized_posterior_probs = unnormalized_posterior(laplace_samples)\n",
    "Z = ... # Estimate Z with importance sampling.\n",
    "\n",
    "# If implemented correctly, Z should be approximately: tensor(0.1154)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae6a99-b614-4172-99cf-17ec20abac9a",
   "metadata": {},
   "source": [
    "Then we can estimate the posterior mean with importance sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_sampling_mean = ... # Estimate the mean with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean as estimated by laplace + importance sampling: \", importance_sampling_mean.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefb3c7-2077-478a-ae7e-f69a9e54c50a",
   "metadata": {},
   "source": [
    "This is a very good estimate of the true posterior mean. Congratualtions, you are done with the first part of the programming exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d8355",
   "metadata": {},
   "source": [
    "# Gibbs sampling for the Ising model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49c417-318f-47af-a4ad-449f1294c74f",
   "metadata": {},
   "source": [
    "The Ising model is a mathematical model of ferromagnetism in statistical mechanics. The model consists of discrete variables that represent magnetic dipole moments of atomic \"spins\" that can be in one of two states (+1 or −1). The spins are arranged in a lattice, allowing each spin to interact with its neighbors. Neighboring spins that agree have a lower energy than those that disagree; the system tends to the lowest energy but heat disturbs this tendency, thus creating the possibility of different structural phases. The image illustrates an ising model on a 2D-lattice:\n",
    "\n",
    "<img src=\"ising_model.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "[Image source](https://www.researchgate.net/figure/Schematic-representation-of-a-configuration-of-the-2D-Ising-model-on-a-square-lattice_fig2_321920877)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62023f33-4040-4511-a85a-137ec162aaf1",
   "metadata": {},
   "source": [
    "The probability of a particular state $\\sigma$ (i.e., a set of spins) in this system is given by:\n",
    "\n",
    "$p(\\sigma) = \\frac{\\exp(-\\beta H(\\sigma))}{Z}$.\n",
    "\n",
    "The normlization constant $Z$ is computationally expensive to evaluate. $H(\\sigma)$ is the hamiltonian defined as:\n",
    "\n",
    "$H(\\sigma) = -\\sum_{\\langle i,j \\rangle} J_{i,j}\\sigma_i\\sigma_j - \\mu \\sum_j h_j \\sigma_j$\n",
    "\n",
    "The notation $\\langle i,j \\rangle$ indicates that $i,j$ are neighbours. $J$ models the interactions between neighbours. We will assume that the magnetic moment $\\mu$ is zero and that J is one:\n",
    "\n",
    "$H(\\sigma) = -\\sum_{\\langle i,j \\rangle} \\sigma_i\\sigma_j$\n",
    "\n",
    "This model (in a circular version, so last spin's right neighbour is the first spin) and a 1D-lattice is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bcb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 10\n",
    "\n",
    "J = 1. # energy due to local field from nearby spins --> corresponds to diffusion term in Hamiltonian\n",
    "h = 0. # energy due to external field --> corresponds to drift term in Hamiltonian\n",
    "\n",
    "def hamiltonian(sigma):\n",
    "#     Hamiltonian = H = -J * (Sum over pairs of nearest neighbour spins) - h (sum over spins).\n",
    "#     Assume 1D lattice, so nearest neighbours are on the right and left of each spin.\n",
    "#     Assume that lattice is circular, so last spin's right neighbour is the first spin.\n",
    "    assert sigma.dim() == 1\n",
    "    nn_left = torch.roll(sigma, -1) # roll lattice to get all left neighbours\n",
    "    nn_right = torch.roll(sigma, 1) # roll lattice to get all right neighbours\n",
    "    term1 = nn_left.dot(sigma)\n",
    "    term2 = nn_right.dot(sigma)\n",
    "    \n",
    "    term3 = sigma.sum()\n",
    "    return -J * (term1 + term2) - h * term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ising_prob(sigma, beta):\n",
    "    return torch.exp(-beta * hamiltonian(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8249fb6-e23e-41db-a5df-0eaee2c412a5",
   "metadata": {},
   "source": [
    "In this tutorial, we will investigate how one can sample states $\\sigma$ given the unnormalized density function $\\tilde{p}(\\sigma) \\propto \\exp(-\\beta H(\\sigma))$. We will do so with Gibbs-sampling.\n",
    "\n",
    "Each value of the initial state is sampled from a discrete distribution that has $p(1) = 0.5$ and $p(-1) = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fd53a-8f6a-4efd-a2c4-e0cf639e3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = torch.as_tensor(torch.randint(2, (num_dim,)) * 2 - 1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6493b93-e2b8-4f3d-bbef-a5daf0e80a21",
   "metadata": {},
   "source": [
    "We then need a function that evaluates the conditional distribution $\\tilde{p}(\\sigma_i | \\sigma_{j \\neq i} = \\overline{\\sigma}_{j \\neq i})$ given the current state $\\overline{\\sigma}$. To obtain this, first write functions that evaluate $\\tilde{p}(\\sigma_i = 1, \\sigma_{j \\neq i} = \\overline{\\sigma}_{j \\neq i})$ and $\\tilde{p}(\\sigma_i = -1, \\sigma_{j \\neq i} = \\overline{\\sigma}_{j \\neq i})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8595f16-138a-496f-a6b3-78cf589f4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_dist_at_plus_1(sigma, dim, beta):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "def joint_dist_at_minus_1(sigma, dim, beta):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "# If implemented correctly, the results should be approximately:\n",
    "# conditional_dist_at_plus_1(torch.ones(10), 0, 0.1) -> tensor(7.3891)\n",
    "# conditional_dist_at_minus_1(torch.ones(10), 0, 0.1) -> tensor(3.3201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e61df-f095-4eee-b90e-00f86e8d9992",
   "metadata": {},
   "source": [
    "Using these functions, write a function that evaluates $\\tilde{p}(\\sigma_i = 1 | \\sigma_{j \\neq i})$. Note that $\\tilde{p}(\\sigma_i = 1 | \\sigma_{j \\neq i}) \\propto \\tilde{p}(\\sigma_i = 1, \\sigma_{j \\neq i})$, i.e., all you have to do is normalize the values of the above functions such that they sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8d367e-a14a-48d4-a31c-554689d13aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_dist_at_plus_1(sigma, dim, beta):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "# If implemented correctly, the results should be approximately:\n",
    "# conditional_dist_at_plus_1(torch.ones(10), 0, 0.1) -> tensor(0.6900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d92e4-d990-4a04-8402-8bf44ad8b948",
   "metadata": {},
   "source": [
    "Finally, write a function that uses `conditional_dist_at_plus_1` to sample the conditional distribution $\\tilde{p}(\\sigma_i | \\sigma_{j \\neq i})$ using `torch.distributions.Bernoulli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c46807-c031-4a0c-ae3d-066a1b43003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditional(sigma, dim, beta):\n",
    "    ...\n",
    "    return conditional_samples # Should have shape (1,)\n",
    "\n",
    "# E.g. sample_conditional(torch.ones(10), 1, 0.5) -> tensor([1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95154d8-a5c3-42a6-adb0-6e63c927c353",
   "metadata": {},
   "source": [
    "Finally, write a function that loops over every dimension and samples each conditional once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d4113-2b94-4eab-99ca-583f11f0a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(sigma, beta):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c84c7-5030-405f-968c-de4a349bde6a",
   "metadata": {},
   "source": [
    "Done! We have implemented the Gibbs-sampler for the Ising model. We can now simulate the Ising model for different values of $\\beta$. We will start with low temperatures, i.e., high values of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa6f54-718e-4e33-8ec0-2f4fd4fc8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ee56e-00d6-4fc0-99ac-985f565b16ce",
   "metadata": {},
   "source": [
    "Run the gibbs sampler for $N$ steps. Visualize the states as an $[N, D]$ matrix, with $D$ being the number of states of the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "for _ in range(20):\n",
    "    s = gibbs(initial, beta=beta)\n",
    "    all_samples.append(deepcopy(s))\n",
    "all_samples = torch.stack(all_samples)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "_ = ax.imshow(all_samples.numpy())\n",
    "ax.set_xlabel(\"State\")\n",
    "ax.set_ylabel(\"Gibbs iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b700e5b-0f4a-4d06-bdff-df0fb1ee8f49",
   "metadata": {},
   "source": [
    "We can check whether samples returned by the gibbs sampler indeed produce states that have high probability. Compute the average probability of states sampled by the Gibbs sampler and compare them to sampling states independently and uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8130b4-dcfc-4966-b690-f9b91ac63607",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_prob_gibbs = torch.mean(torch.stack([ising_prob(s, beta=beta) for s in all_samples]))\n",
    "print(\"Average (unnormalized) probability of gibbs samples:\", average_prob_gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4cfa0-1517-4002-9490-b9f49b4a338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples = torch.as_tensor(torch.randint(2, (50, num_dim)) * 2 - 1, dtype=torch.float32)\n",
    "average_prob_random = torch.mean(torch.stack([ising_prob(s, beta=beta) for s in random_samples]))\n",
    "print(\"Average (unnormalized) probability of random samples:\", average_prob_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac31cd6-70be-4241-832c-846301946582",
   "metadata": {},
   "source": [
    "Next, we will run the sampler for high temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2bc66-8beb-44d6-82bb-d9a36f271b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140d153-d95f-4af3-ab9b-5dedf84b8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "for _ in range(20):\n",
    "    s = gibbs(initial, beta=beta)\n",
    "    all_samples.append(deepcopy(s))\n",
    "all_samples = torch.stack(all_samples)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "_ = ax.imshow(all_samples.numpy())\n",
    "_ = ax.set_xlabel(\"State\")\n",
    "_ = ax.set_ylabel(\"Gibbs iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e0ed8-2959-4680-bd6e-62b4cdead69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
