{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from seaborn import pairplot\n",
    "from typing import Tuple\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Gaussian Mixture Model EM-Algorithm\n",
    "\n",
    "In this notebook we will implement the EM-Algorithm for GMMs and apply it to \"real\" dataset: Iris. This is perhaps the best known dataset in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. The are four features, the sepal length in cm the sepal width in cm, the petal length in cm and the petal width in cm. \n",
    "\n",
    "The next two cells will load the dataset into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as dataframe\n",
    "data = load_iris(as_frame=True)\n",
    "\n",
    "df_X = data[\"data\"]\n",
    "df_y = data[\"target\"]\n",
    "\n",
    "# Train data in numpy arrays\n",
    "X = df_X.to_numpy()\n",
    "y = df_y.to_numpy()\n",
    "\n",
    "# Dataset in a df for plotting\n",
    "df = pd.concat([df_X, df_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot(df, hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above pairplot one can already guess that a Gaussian Mixture model can nicely model this data, so let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "Here we will implement the E-step and M-step as functions. You already derived all the formulas for the EM algorithm by yourself in Exercise 2. Here are the solutions you should have arrived at:\n",
    "\n",
    "### E-step\n",
    "\n",
    "This requires to compute\n",
    "$$ r_{ij} = \\frac{\\pi_j^{old} \\mathcal{N}(x_i; \\mu_j^{old}, \\Sigma_j^{old})}{\\sum_{j=1}^k \\pi_j^{old} \\mathcal{N}(x_i; \\mu_j^{old}, \\Sigma_j^{old})} $$\n",
    "for each datapoint. This equals the posterior probabilities of cluster assignments given the current parameter candidates. \n",
    "\n",
    "**NOTE**: For numerical stability, you may want to perform most computations in log-space and perform normalization using the \"logsumexp\" trick (i.e. using np.logsumexp). \n",
    "\n",
    "### M-step \n",
    "\n",
    "The maximization is possible in closed form, and given by:\n",
    "\n",
    "$$R_j = \\sum_{i=1}^nr_{ij} \\qquad   \\pi_j = \\frac{R_j}{n} \\qquad \\mu_j = \\frac{1}{R_j}\\sum_{i=1}^n r_{ij}x_i \\qquad \\Sigma_j = \\frac{1}{R_j} \\sum_{i=1}^n r_{ij}(x_i-\\mu_j) (x_i-\\mu_j)^T$$\n",
    "\n",
    "**NOTE**: There are some numerical considerations\n",
    "* In theory $r_{ij} \\neq 0$ for all $j, i$. Yet in practise, we use floating point numbers which have limited precision. As a result e.g. $R_j$ can be zero if all datapoints are far away from a cluster, which would lead to a division by zero.\n",
    "* The covariance matrix should be symmetric postive definite. It may be required to ensure this i.e. by adding a small value to the diagonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables you may want to use\n",
    "n = X.shape[0] # Number of samples\n",
    "d = X.shape[1] # Dimensionality of each sample\n",
    "\n",
    "# Here we know the number of clusters, there are only 3 types of iris plants in the data.\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, pi, mu, cov) -> np.array:\n",
    "    \"\"\"\n",
    "    Perform the E step\n",
    "\n",
    "    args:\n",
    "        X (n, d): data, n samples of dimensionality d\n",
    "        pi (k): mixture weights of Gaussians\n",
    "        mu (k,d): means of Gaussians\n",
    "        cov (k,d,d): covariances of Gaussians\n",
    "    returns:\n",
    "        r (n,k): posterior probability for cluster assignments\n",
    "                     for each datatpoint\n",
    "    \"\"\" \n",
    "    # Your code here ...\n",
    "  \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for correctness: \n",
    "test1 = E_step(X, np.array([1/3, 1/3, 1/3]), np.ones((3,4)), [np.eye(4) for k in range(3)])\n",
    "test2 = E_step(X, np.array([1/3, 1/3, 1/3]), np.array([[-1.,0.,3., 0.], [0.,2.,0., 1.], [5.,5.,5.,5.]]), [np.eye(4) for k in range(3)])\n",
    "\n",
    "assert test1.shape[0] == n, \"The first dimension should be of shape N\"\n",
    "assert test1.shape[1] == k, \"The second dimension should be of shape K\"\n",
    "assert np.isclose(test1.mean(0), np.array([1/3, 1/3, 1/3]), atol=1e-2, rtol=1e-2).all(), \"If all clusters are the same, the posterior probabilities should be uniform.\"\n",
    "assert np.isclose(test2.mean(0), np.array([2.93392254e-05, 2.85799805e-01, 7.14170855e-01]), atol=1e-2, rtol=1e-2).all(), \"There is something wrong :(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(r, X) -> Tuple[np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Maximize expectation of complete data log likelihood\n",
    "    \n",
    "    args:\n",
    "        r (n,k): posterior probability for cluster assignments\n",
    "                     for each datatpoint\n",
    "    returns:\n",
    "        X (n, d): data, n samples of dimensionality d\n",
    "        pi (k): mixture weights of Gaussians\n",
    "        mu (k,d): means of Gaussians\n",
    "        cov (k,d,d): covariances of Gaussians\n",
    "    \"\"\" \n",
    "    \n",
    "    # Your code here\n",
    "   \n",
    "\n",
    "    return pi, mu, covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for correctness\n",
    "pi_test1, mu_test1, cov_test1 = M_step(test1, X)\n",
    "assert pi_test1.shape[0] == k, \"There should be one mixture coefficient for each cluster K.\"\n",
    "assert mu_test1.shape[0] == k and mu_test1.shape[1] == d, \"The shape of the cluster means should be (K, d).\"\n",
    "assert cov_test1.shape[0] == k and cov_test1.shape[1] == d and cov_test1.shape[2] == d, \"The shape of the cluster covariance matrices should be (K,d,d)\"\n",
    "assert np.isclose(pi_test1.sum(), 1., atol=1e-3, rtol=1e-3), \"The mixture coefficients must sum to one.\"\n",
    "assert all([np.all(np.linalg.eigvalsh(cov_test1[j]) > 0.) for j in range(k)]), \"The estimated covariance matrix is not positive definite\"\n",
    "assert np.isclose(pi_test1, np.array([1/3, 1/3, 1/3]), atol=1e-2, rtol=1e-2).all() and np.isclose(mu_test1.mean(0), np.array([5.84333333, 3.05733333, 3.758     , 1.19933333]), atol=1e-2, rtol=1e-2).all() and np.isclose(cov_test1.mean(), 0.6058022499999997, atol=1e-2, rtol=1e-2) , \"Something went wrong :(\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 \n",
    "\n",
    "While not required for the algorithm, it is a good idea to compute the total data negative loglikelihood. Recall that for a Gaussian Mixture model this is given by\n",
    "\n",
    "$$ -\\ln p(x) =-\\sum_{i=1}^n \\ln \\sum_{j=1}^k \\pi_j \\mathcal{N}(x_i; \\mu_j, \\Sigma_j) $$\n",
    "\n",
    "**NOTE**: As we saw in the EXAMple question each iteration must decrease the negative loglikelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeloglikelihood(X, pi, mu, cov) -> float:\n",
    "    \"\"\"\n",
    "    Computes the marginal negative log likelihood.\n",
    "    \n",
    "    args:\n",
    "        X (n, d): data, n samples of dimensionality d\n",
    "        pi (k): mixture weights of Gaussianss\n",
    "        mu (k,d): means of Gaussians\n",
    "        cov (k,d,d): covariances of Gaussians\n",
    "    \"\"\" \n",
    "    # Your code here\n",
    "  \n",
    "    \n",
    "    return -logll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for correctness\n",
    "assert np.isclose(negativeloglikelihood(X, pi_test1, mu_test1, cov_test1), 379.914630122269, atol=1e-2, rtol=1e-2), \"Something is wrong :(\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3\n",
    "\n",
    "Implement an initialization scheme. As you learned in the lecture, EM will only converge to a **local** extrema. Thus to find a good solution you should implement a good initialization or run the algorithm multiple times on different random initializations.\n",
    "\n",
    "Here we provide some suggestions, but you are free to try out your own initializations!\n",
    "* You can use a simple clustering algorithm e.g. KMeans to find good initializations. Be free to use any implementation you want, e.g. from sklearn (already imported).\n",
    "* You can derive one from the data. \n",
    "* You can use a random initialization. Note if it is \"bad\" then you may run into numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    \"\"\"\n",
    "   Some initialization scheme\n",
    "   \n",
    "   returns:\n",
    "        pi (k): mixture weights of Gaussians\n",
    "        mu (k,d): means of Gaussians\n",
    "        cov (k,d,d): covariances of Gaussians\n",
    "    \"\"\" \n",
    "    # Your code here\n",
    "  \n",
    "    return pi, mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests for correctness\n",
    "pi_test2, mu_test2, cov_test2 = init_params()\n",
    "assert np.isclose(pi_test2.sum(), 1., atol=1e-3, rtol=1e-3), \"The mixture coefficients must sum to one.\"\n",
    "assert all([np.all(np.linalg.eigvalsh(cov_test2[j]) > 0.) for j in range(k)]), \"The estimated covariance matrix is not positive definite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5\n",
    "\n",
    "Implement the EM algorithm using the functions you just implemented above.\n",
    "\n",
    "Run the algorithm on the data, the following cells should plot the \"loss\" history. Recall, if everything works this should be monotonically decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(pi, mu, cov, iters=50):\n",
    "    \"\"\"\n",
    "    \n",
    "    A function that runs the EM algorithm.\n",
    "\n",
    "    args:\n",
    "        pi (k): Initial mixture coefficients.\n",
    "        mu (k,d): Initial cluster means.\n",
    "        cov (k,d,d): Initial cluster covariance matrices.\n",
    "        iters (int, optional): Number of iterations. Defaults to 50.\n",
    "\n",
    "    returns:\n",
    "        pi (k): Final mixture coefficients.\n",
    "        mu (k,d): Final cluster means.\n",
    "        cov (k,d,d): Final cluster covariance matrices.\n",
    "        loss_hist: A list containing the negative loglikelihoods \n",
    "                   at each iteration\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "\n",
    "    return pi, mu, cov, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu, cov, loss = fit(*init_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.7\n",
    "\n",
    "Note that for now, we did not use the labels $y$. This is because our probabilistic model is *unsupervised*. We assume there is an underlying \"clustered\" structure but pretend we don't know it. This raises the question if we did recover the ground truth labels.\n",
    "\n",
    "To test this, predict and plot the most likely cluster assignments for the data.\n",
    "\n",
    "**NOTE** You already implemented the functions you require to do this.\n",
    "\n",
    "Compere it to the plot on top using the ground truth cluster assignments. Does it recover the truth? If not, give some reasons why the approach may failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "# The predictions should be an array of length n containing either 0,1 or 2.\n",
    "df[\"predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot(df.drop(\"target\", axis=1), hue=\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also compare your own implementation to Sklearn's\n",
    "You could compare your own implementation to Sklearn's. This library uses K means as initialisation, so in case you used a different scheme, your results might look different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, max_iter=50).fit(X)\n",
    "gmm_scores = gmm.score_samples(X)\n",
    "\n",
    "predictions = probs.argmax(-1)\n",
    "\n",
    "# The predictions should be an array of length N containing either 0,1 or 2.\n",
    "df[\"predictions\"] = predictions\n",
    "pairplot(df.drop(\"target\", axis=1), hue=\"predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
