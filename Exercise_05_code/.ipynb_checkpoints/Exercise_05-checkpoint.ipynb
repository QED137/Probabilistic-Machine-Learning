{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The focus of this week is implementing the Laplace approximation to the logistic regression problem described in lectures.\n",
    "\n",
    "## Overview\n",
    "- **Task 1**: Calculate the Laplace Approximation to the posterior distribution\n",
    "- **Task 2**: Sample from the Laplace Approximation to plot decision boundaries\n",
    "- **Epilogue**: Use automatic differentiation (autodiff) to calculate the gradient and Hessian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import scipy.optimize\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the required data\n",
    "\n",
    "Load the data saved in the file `observed_data.p` (e.g., using `pickle`). It contains your observation in the form of `X` values and `y` binary labels as type `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your code goes here #####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_data = ...\n",
    "# y_data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Let us plot the data in the two classes  to see if we can separate the classes visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(X_data[y_data==0,0], X_data[y_data==0,1], color='orange', label='y=0')\n",
    "ax.scatter(X_data[y_data==1,0], X_data[y_data==1,1], color='darkviolet', marker=\"+\" ,s=100,label='y=1') \n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dummy X variable\n",
    "\n",
    "We might want to learn an offset as well for the decision boundary, instead of just $w_1x + w_2y$. Modify `X_data` so that the first row is filled by 1's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = #Your code here...\n",
    "y = y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Define Logistic Regression Model\n",
    "\n",
    "Recall from the lectures the likelihood model for logistic regression with weights $w$.\n",
    "\n",
    "\\begin{equation}\n",
    "p(y|w,x) = \\left\\{\n",
    "    \\begin{split}\n",
    "    \\sigma(x^\\top w) \\quad & y=1\\\\\n",
    "    1-\\sigma(x^\\top w) \\quad & y=0\n",
    "    \\end{split}\n",
    "     \\right.\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\sigma$ is the sigmoid function\n",
    "\\begin{equation}\n",
    "\\sigma(z) = \\frac{1}{1+\\exp(-z)}\n",
    "\\end{equation}\n",
    "\n",
    "Recall that assuming that the different datapoints $(x,y)$ are independent, then $p(Y|w,X) = \\prod_{i=1}^{N}p(y_i|w.x_i)$.\n",
    "\n",
    "Implement the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n",
    "\n",
    "def log_likelihood(w, X, y):\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Laplace Approximation\n",
    "\n",
    "Now, let us move on to the task of approximating a posterior distribution. Assumke we define a Gaussian prior over the weights:\n",
    "\\begin{equation}\n",
    "p(w) = \\mathcal{N}(w; 0, \\sigma^{2}_{w}I)\n",
    "\\end{equation}\n",
    "\n",
    "Then recall that the Laplace approximation is given by $p(w|Y,X) \\approx \\mathcal{N}(w; w_{*}, -\\mathcal{H}^{-1})$, and is found in two steps:\n",
    "- **Step 1**: Calculate the mode $w_{*}$ of $\\log p(w|Y,X)$ by gradient ascent.\n",
    "- **Step 2**: Compute the Hessian matrix at the mode:\n",
    "\\begin{equation}\n",
    "\\mathcal{H} = \\nabla\\nabla \\log p(w|Y,X)|_w{*}\n",
    "\\end{equation}\n",
    "\n",
    "Implement the functions below to calculate the negative log posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Laplace Approximation\n",
    "prior_sigma = 1.0\n",
    "\n",
    "\n",
    "\n",
    "def log_prior(w):\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n",
    "\n",
    "def neg_log_posterior(w, X, y):\n",
    "    # your code here...\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize the negative log posterior to find the MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "laplace_opt=scipy.optimize.minimize(neg_log_posterior,w,args=(X,y),jac=False,options={'maxiter':25})\n",
    "map = laplace_opt.x\n",
    "print(map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results\n",
    "\n",
    "Let's plot the predictions using the MAP (choosing the class that is most likely, even if the predicted probability is close to 0.5). Let's also plot whether this prediction matches the true label or not to see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP predictions\n",
    "\n",
    "y_pred = np.round(sigmoid(X.dot(map)))\n",
    "fig,ax = plt.subplots(figsize=(10,5),ncols=2,sharey=True)\n",
    "ax[0].scatter(X_data[y_pred==0,0], X_data[y_pred==0,1], color='orange', label='y=0')\n",
    "ax[0].scatter(X_data[y_pred==1,0], X_data[y_pred==1,1], color='darkviolet', marker=\"+\" ,s=100,label='y=1')\n",
    "ax[0].set_xlabel('X1')\n",
    "ax[0].set_ylabel('X2')\n",
    "ax[0].legend()\n",
    "ax[1].scatter(X_data[y_data==y_pred,0], X_data[y_data==y_pred,1], color='green', label='correct')\n",
    "ax[1].scatter(X_data[y_data!=y_pred,0], X_data[y_data!=y_pred,1], color='red', label='incorrect')\n",
    "ax[1].set_xlabel('X1')\n",
    "ax[1].set_ylabel('X2')\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Hessian matrix\n",
    "\n",
    "Complete the function below and evaluate the matrix at the MAP calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(w, X, y):\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "H = hessian(map, X, y)\n",
    "\n",
    "post_mean = map\n",
    "post_cov = np.linalg.inv(-H)\n",
    "\n",
    "print(f'Posterior mean: {post_mean}')\n",
    "print(f'Posterior covariance: {post_cov}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Sample from Posterior and visualize the results\n",
    "\n",
    "One way to visualize the posterior is to plot the decision boundaries associated with samples from the posterior.\n",
    "\n",
    "The predicted probability of both classes will be exactly 0.5 when $\\sigma(x^{\\top}w) = 0.5$, which holds when $x^{\\top}w= 0$. Recalling that we introduced a dummy variable at the beginning of this notebook, we expand the above product as $w_0 + w_1x_1 + w_2x_2 =0$.\n",
    "\n",
    "Therefore, for every sample from the posterior $w\\sim p(w|Y,X)$, we can plot the corresponding decision boundary by plotting the line $x_2 = \\frac{-w_0 - w_1x_1}{w_2}$. We can also color each decision boundary proportional to the posterior probability of $w$.\n",
    "\n",
    "Implement the missing code blocks below to generate the decision boundary plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.cm.get_cmap('winter')\n",
    "\n",
    "\n",
    "\n",
    "def draw_posterior_sample(mean,cov):\n",
    "    # This function should return one sample from the posterior distribution\n",
    "    # Hint: use np.random.multivariate_normal\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n",
    "def post_prob(w,mean,cov):\n",
    "    # This function should return the probability of the sample w under the posterior distribution\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n",
    "\n",
    "# normalzing values to [0,1] for color mapping\n",
    "# What is the maximum probability of the posterior distribution?\n",
    "# (Hint: max value is achieved at the MAP).\n",
    "w_prob_max = #your code here...\n",
    "\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "x1 = np.linspace(-4,6,100) #fixed array of first coordinate x values to plot lines with\n",
    "for i in range(10):\n",
    "    w_post = draw_posterior_sample(post_mean,post_cov)\n",
    "    w_prob = post_prob(w_post,post_mean,post_cov)\n",
    "\n",
    "    x2 = (-w_post[0]-w_post[1]*x1)/w_post[2] #Calculate the x2 values for the decision boundary\n",
    "    ax.plot(x1,x2,c = cmap(w_prob/w_prob_max)) #Plot the line and color it according to the probability of w\n",
    "\n",
    "# Plot the dataset as before\n",
    "ax.scatter(X_data[y_data==0,0], X_data[y_data==0,1], color='orange', label='y=0',zorder=-1)\n",
    "ax.scatter(X_data[y_data==1,0], X_data[y_data==1,1], color='darkviolet', marker=\"+\" ,s=100,label='y=1',zorder=-1) \n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.legend()\n",
    "ax.set_ylim(-4,7)\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin=0, vmax=1), cmap=cmap), ax=ax, label='prob');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue - Automatic Differentiation\n",
    "\n",
    "In this exercise, we calculated analytic expressions for the Hessian matrix (and did not explicitly used the analytic derivative to solve the optimization problem to find the the MAP, although this would have made the task even easier). \n",
    "\n",
    "Sometimes, analytic expressions are not possible to obtain. However, using automatic differentiation, we can compute derivatives when we calculate the value of the log-posterior, without providing analytical expressions!\n",
    "\n",
    "Here, we will reimplement the functions from Task 1, but now using an automatic differentiation framework, and show how we can calculate the derivative (and the Hessian) of the log-posterior. \n",
    "\n",
    "We will use [Pytorch](https://pytorch.org/get-started/locally/) for this demonstration, so make sure you have a local installation. There are other automatic differentiation libraries in python, namely [JAX](https://jax.readthedocs.io/en/latest/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.Tensor(X) #Convert X to a torch tensor\n",
    "y_torch = torch.Tensor(y) #Convert y to a torch tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplemeting the log-posterior\n",
    "\n",
    "The functions should be the same as the functions defined at the start of this notebook, but all functions should now be in `torch` - you should not use any numpy functions or arrays here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to re-implement sigmoid, as you can use torch.sigmoid()!\n",
    "\n",
    "\n",
    "def torch_log_likelihood(w, X, y):\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def torch_log_prior(w):\n",
    "    # your code here...\n",
    "    raise NotImplementedError\n",
    "\n",
    "def torch_neg_log_posterior(w, X, y):\n",
    "    #your code here...\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the MAP\n",
    "\n",
    "We can now get the gradient of the log posterior using automatic differentiation! Let's use it to write our own simple optimization loop (as opposed to the previous optimization using `scipy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_torch = torch.tensor([0.1, 0.1, 0.1], requires_grad=True) #Initialize w as a torch tensor\n",
    "optimizer = optim.SGD([map_torch],lr=0.01) #Initialize a basic pytorch optimizer\n",
    "\n",
    "for i in range(200):\n",
    "    optimizer.zero_grad() #make sure we do not carry any gradients from the previous iteration\n",
    "    loss = torch_neg_log_posterior(map_torch, X_torch, y_torch) #Compute the negative log posterior\n",
    "    loss.backward() #Compute the gradient of the negative log posterior\n",
    "    optimizer.step() #Update the weights\n",
    "    if i%10 == 0:\n",
    "        print(\"negative log posterior: \" ,loss.item())\n",
    "\n",
    "# Compare the MAP estimates from the two methods\n",
    "print(\"MAP computed with autoamtic differentiation: \" , map_torch.data)\n",
    "print(\"MAP computed with scipy: \" , map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Hessian\n",
    "\n",
    "Similarly, we can compute the Hessian when computing the log-posterior using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Negative hessian as we calculated the negative log posterior\n",
    "neg_hessian = torch.autograd.functional.hessian(torch_neg_log_posterior, (map_torch,X_torch,y_torch), create_graph=True)\n",
    "# We actually get the hessian for all inputs, (so also for X and y!). We only care about the Hessian for the weights.\n",
    "H_torch = -neg_hessian[0][0]\n",
    "\n",
    "# Compare the Hessian with the one you computed before\n",
    "print(\"Analytical Hessian:\")\n",
    "print(H)\n",
    "print(\"PyTorch Hessian:\")\n",
    "print(H_torch.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
